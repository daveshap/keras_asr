{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras ASR Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9FGvvZk9OUs"
   },
   "source": [
    "## Install Prerequisites\n",
    "Install a bunch of stuff and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKzHlVuNQ5t8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!apt-get -qq update\n",
    "#!apt-get -qq install -y libsndfile-dev wget p7zip-full\n",
    "#!pip install SoundFile -q\n",
    "#!pip install librosa -q\n",
    "#!pip install tensorflow-hub -q\n",
    "#!pip install seaborn -q\n",
    "#!pip install keras -q\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heTM3RWITJ1s"
   },
   "source": [
    "## Import Modules\n",
    "Here we import modules and define a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fpVMMB0HOELl",
    "outputId": "6dd02bb3-a80f-43ac-91cb-7e3b3156431e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import librosa\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import random\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "\n",
    "mfcc_features = 128\n",
    "max_len = 1021\n",
    "root_dir = './'\n",
    "output_dir = './speech_data'\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iByogy6X6Vvh"
   },
   "source": [
    "## Generate MFCC\n",
    "Produces pickled dictionaries with the following keys:\n",
    "* label (sentence string)\n",
    "* sample (MFCC as numpy ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24IYMyzGTrEi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking all directories\n",
      "5831\n",
      "('FOLDER:', './train-clean-100/4406/16882')\n",
      "('FOLDER:', './train-clean-100/4406/16883')\n",
      "('FOLDER:', './train-clean-100/7447/91187')\n",
      "('FOLDER:', './train-clean-100/7447/91186')\n",
      "('FOLDER:', './train-clean-100/3699/19402')\n",
      "('FOLDER:', './train-clean-100/3699/47246')\n",
      "('FOLDER:', './train-clean-100/3699/175950')\n",
      "('FOLDER:', './train-clean-100/3699/19401')\n",
      "('FOLDER:', './train-clean-100/4397/15666')\n",
      "('FOLDER:', './train-clean-100/4397/15668')\n",
      "('FOLDER:', './train-clean-100/4397/15678')\n",
      "('FOLDER:', './train-clean-100/3857/182317')\n",
      "('FOLDER:', './train-clean-100/3857/182315')\n",
      "('FOLDER:', './train-clean-100/3857/180923')\n",
      "('FOLDER:', './train-clean-100/8098/275181')\n",
      "('FOLDER:', './train-clean-100/8098/278252')\n",
      "('FOLDER:', './train-clean-100/8098/278278')\n",
      "('FOLDER:', './train-clean-100/5463/39174')\n",
      "('FOLDER:', './train-clean-100/5463/39173')\n",
      "('FOLDER:', './train-clean-100/8123/275216')\n",
      "('FOLDER:', './train-clean-100/8123/275193')\n",
      "('FOLDER:', './train-clean-100/8123/275209')\n",
      "('FOLDER:', './train-clean-100/5456/58161')\n",
      "('FOLDER:', './train-clean-100/5456/62043')\n",
      "('FOLDER:', './train-clean-100/5456/62014')\n",
      "('FOLDER:', './train-clean-100/5456/24741')\n",
      "('FOLDER:', './train-clean-100/8630/305213')\n",
      "('FOLDER:', './train-clean-100/8630/305212')\n",
      "('FOLDER:', './train-clean-100/6209/34599')\n",
      "('FOLDER:', './train-clean-100/6209/34601')\n",
      "('FOLDER:', './train-clean-100/6209/34600')\n",
      "('FOLDER:', './train-clean-100/2007/132570')\n",
      "('FOLDER:', './train-clean-100/2007/149877')\n",
      "('FOLDER:', './train-clean-100/839/130898')\n",
      "('FOLDER:', './train-clean-100/1743/142913')\n",
      "('FOLDER:', './train-clean-100/1743/142914')\n",
      "('FOLDER:', './train-clean-100/1743/142912')\n",
      "('FOLDER:', './train-clean-100/2518/154825')\n",
      "('FOLDER:', './train-clean-100/2518/154826')\n",
      "('FOLDER:', './train-clean-100/32/4137')\n",
      "('FOLDER:', './train-clean-100/32/21631')\n",
      "('FOLDER:', './train-clean-100/32/21625')\n",
      "('FOLDER:', './train-clean-100/32/21634')\n",
      "('FOLDER:', './train-clean-100/27/124992')\n",
      "('FOLDER:', './train-clean-100/27/123349')\n",
      "('FOLDER:', './train-clean-100/200/124140')\n",
      "('FOLDER:', './train-clean-100/200/126784')\n",
      "('FOLDER:', './train-clean-100/200/124139')\n",
      "('FOLDER:', './train-clean-100/5867/48852')\n",
      "('FOLDER:', './train-clean-100/909/131041')\n",
      "('FOLDER:', './train-clean-100/909/131044')\n",
      "('FOLDER:', './train-clean-100/909/131045')\n",
      "('FOLDER:', './train-clean-100/1098/133695')\n",
      "('FOLDER:', './train-clean-100/5390/30096')\n",
      "('FOLDER:', './train-clean-100/5390/24512')\n",
      "('FOLDER:', './train-clean-100/5390/30102')\n",
      "('FOLDER:', './train-clean-100/8468/286673')\n",
      "('FOLDER:', './train-clean-100/8468/295198')\n",
      "('FOLDER:', './train-clean-100/8468/294887')\n",
      "('FOLDER:', './train-clean-100/7505/83618')\n",
      "('FOLDER:', './train-clean-100/7505/258964')\n",
      "('FOLDER:', './train-clean-100/7505/258958')\n",
      "('FOLDER:', './train-clean-100/40/222')\n",
      "('FOLDER:', './train-clean-100/40/121026')\n",
      "('FOLDER:', './train-clean-100/201/127786')\n",
      "('FOLDER:', './train-clean-100/201/122255')\n",
      "('FOLDER:', './train-clean-100/5339/14134')\n",
      "('FOLDER:', './train-clean-100/5339/14133')\n",
      "('FOLDER:', './train-clean-100/7226/86964')\n",
      "('FOLDER:', './train-clean-100/7226/86965')\n",
      "('FOLDER:', './train-clean-100/250/140277')\n",
      "('FOLDER:', './train-clean-100/250/142286')\n",
      "('FOLDER:', './train-clean-100/250/142276')\n",
      "('FOLDER:', './train-clean-100/322/124146')\n",
      "('FOLDER:', './train-clean-100/322/124147')\n",
      "('FOLDER:', './train-clean-100/5652/19215')\n",
      "('FOLDER:', './train-clean-100/5652/39938')\n",
      "('FOLDER:', './train-clean-100/8465/246943')\n",
      "('FOLDER:', './train-clean-100/8465/246947')\n",
      "('FOLDER:', './train-clean-100/8465/246942')\n",
      "('FOLDER:', './train-clean-100/8465/246940')\n",
      "('FOLDER:', './train-clean-100/1235/135884')\n",
      "('FOLDER:', './train-clean-100/1235/135883')\n",
      "('FOLDER:', './train-clean-100/1235/135887')\n",
      "('FOLDER:', './train-clean-100/1246/124550')\n",
      "('FOLDER:', './train-clean-100/1246/124548')\n",
      "('FOLDER:', './train-clean-100/1246/135815')\n",
      "('FOLDER:', './train-clean-100/8108/274318')\n",
      "('FOLDER:', './train-clean-100/8108/280359')\n",
      "('FOLDER:', './train-clean-100/8108/280354')\n",
      "('FOLDER:', './train-clean-100/831/130746')\n",
      "('FOLDER:', './train-clean-100/831/130739')\n",
      "('FOLDER:', './train-clean-100/6019/3185')\n",
      "('FOLDER:', './train-clean-100/7059/77900')\n",
      "('FOLDER:', './train-clean-100/7059/77897')\n",
      "('FOLDER:', './train-clean-100/7059/88364')\n",
      "('FOLDER:', './train-clean-100/87/121553')\n",
      "('FOLDER:', './train-clean-100/1963/142776')\n",
      "('FOLDER:', './train-clean-100/1963/147036')\n",
      "('FOLDER:', './train-clean-100/1963/142393')\n",
      "('FOLDER:', './train-clean-100/3440/171009')\n",
      "('FOLDER:', './train-clean-100/3440/171006')\n",
      "('FOLDER:', './train-clean-100/1069/133709')\n",
      "('FOLDER:', './train-clean-100/1069/133699')\n",
      "('FOLDER:', './train-clean-100/2384/152900')\n",
      "('FOLDER:', './train-clean-100/2764/36619')\n",
      "('FOLDER:', './train-clean-100/2764/36616')\n",
      "('FOLDER:', './train-clean-100/2764/36617')\n",
      "('FOLDER:', './train-clean-100/2002/139469')\n",
      "('FOLDER:', './train-clean-100/1841/150351')\n",
      "('FOLDER:', './train-clean-100/1841/159771')\n",
      "('FOLDER:', './train-clean-100/1841/179183')\n",
      "('FOLDER:', './train-clean-100/3259/158083')\n",
      "('FOLDER:', './train-clean-100/6529/62556')\n",
      "('FOLDER:', './train-clean-100/6529/62554')\n",
      "('FOLDER:', './train-clean-100/4195/17507')\n",
      "('FOLDER:', './train-clean-100/4195/186238')\n",
      "('FOLDER:', './train-clean-100/4195/186236')\n",
      "('FOLDER:', './train-clean-100/4195/186237')\n",
      "('FOLDER:', './train-clean-100/7780/274562')\n",
      "('FOLDER:', './train-clean-100/625/132118')\n",
      "('FOLDER:', './train-clean-100/625/132112')\n",
      "('FOLDER:', './train-clean-100/7113/86041')\n",
      "('FOLDER:', './train-clean-100/6437/66173')\n",
      "('FOLDER:', './train-clean-100/6437/66172')\n"
     ]
    }
   ],
   "source": [
    "print 'walking all directories'\n",
    "everything = os.walk('./')\n",
    "data_dirs = []\n",
    "for i in everything:\n",
    "    if len(i[2]) > 2:\n",
    "        data_dirs.append({'path': i[0], 'files': i[2]})\n",
    "#for d in data_dirs:\n",
    "#    print(d['path'])\n",
    "print(len(data_dirs))\n",
    "    \n",
    "# TODO remove data_dirs that have already been processed into the output folder\n",
    "\n",
    "    \n",
    "for folder in data_dirs:\n",
    "    samples = []\n",
    "    labels = []\n",
    "    print('FOLDER:', folder['path'])\n",
    "    for file in folder['files']:\n",
    "        if 'flac' in file:\n",
    "            with open(folder['path'] + '/' + file, 'rb') as f:\n",
    "                data, samplerate = sf.read(f)\n",
    "                sample = librosa.feature.melspectrogram(y=data, sr=samplerate)\n",
    "                #sample = librosa.feature.mfcc(y=data, sr=samplerate)\n",
    "                samples.append({'data': sample, 'file': file})\n",
    "        if 'txt' in file:\n",
    "            with open(folder['path'] + '/' + file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                labels = lines\n",
    "    #print(samples)\n",
    "    #print(labels)\n",
    "    for entry in labels:\n",
    "        file = entry.split(' ')[0]\n",
    "        label = entry.replace(file, '').replace('\\\\n', '').strip().lower()\n",
    "        for record in samples:\n",
    "            if file in record['file']:\n",
    "                final = {'label': label, 'sample': record['data']}\n",
    "                with open(output_dir + '/' + file + '.pickle', 'wb') as outfile:\n",
    "                    pickle.dump(final, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTSy_Hwt7mAI"
   },
   "source": [
    "## Generate Embeddings\n",
    "This step uses Google's Universal Sentence Encoder to generate **semantic vectors** from the sentence labels. Semantic vectors are consistently of size 512 and can represent any chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYG0kztv7koo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0525d8ec36f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \"\"\"\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embed = hub.Module(module_url)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "files = os.listdir(output_dir)\n",
    "max_len = 0\n",
    "chunk_size = 100\n",
    "idx = 0\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    for chunk in chunks(files, 100):\n",
    "        print('loading new chunk of files')\n",
    "        print('max length is:', max_len)\n",
    "        sentences = []\n",
    "        for file in chunk:\n",
    "            with open(output_dir + '/' + file, 'rb') as infile:\n",
    "                d = pickle.load(infile)\n",
    "            if d['embedding']:\n",
    "                continue\n",
    "            sentences.append(d['label'])\n",
    "            if d['sample'].shape[1] > max_len:\n",
    "                max_len = d['sample'].shape[1]\n",
    "        encoded = session.run(embed(sentences))\n",
    "        print('encoded', len(encoded), 'sentences')\n",
    "        print('saving samples with embeddings')\n",
    "        idx = 0\n",
    "        for file in chunk:\n",
    "            with open(output_dir + '/' + file, 'rb') as infile:\n",
    "                d = pickle.load(infile)\n",
    "                d['embedding'] = encoded[idx]\n",
    "                idx += 1\n",
    "            with open(output_dir + '/' + file, 'wb') as outfile:\n",
    "                pickle.dump(d, outfile)\n",
    "\n",
    "print('completed data prep!')\n",
    "\n",
    "with open(output_dir + '/' + files[10], 'rb') as testfile:\n",
    "  d = pickle.load(testfile)\n",
    "print('EXAMPLE FILE:')\n",
    "print('LABEL:     ', type(d['label']), d['label'])\n",
    "print('SAMPLE:    ', type(d['sample']), d['sample'].shape)\n",
    "print('EMBEDDING: ', type(d['embedding']), d['embedding'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AJV7aoQojCv"
   },
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "dxnGyV83eSSl",
    "outputId": "2d238213-698a-4941-97f5-24686dc7126c"
   },
   "outputs": [],
   "source": [
    "print('composing model')\n",
    "encoder = Sequential()\n",
    "\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(128, 1021, 1)))\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(Dropout(0.1))\n",
    "\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(Dropout(0.1))\n",
    "\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(Dropout(0.1))\n",
    "\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(Dropout(0.1))\n",
    "\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(Dropout(0.1))\n",
    "\n",
    "encoder.add(Flatten())\n",
    "encoder.add(Dense(512, activation='softmax'))\n",
    "encoder.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(encoder.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fA1GElneEBcE"
   },
   "source": [
    "## Train Encoder\n",
    "The encoder is the heavy lifter. It looks at raw audio data in the form of an MFCC and encodes it to a semantic vector. Because we have so much data it won't all fit into GPU memory therefore we have to generate smaller batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1781
    },
    "colab_type": "code",
    "id": "r5YOGSO0ERDD",
    "outputId": "c12caf28-5d23-4089-be34-9274151ed116"
   },
   "outputs": [],
   "source": [
    "print('compiling data')\n",
    "files = os.listdir(output_dir)\n",
    "batch_size = 128\n",
    "steps_per_epoch = int(len(files) / batch_size)\n",
    "epochs = 10\n",
    "\n",
    "def data_generator(sample_count):\n",
    "    random.seed()\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for c in range(sample_count):\n",
    "        idx = random.randint(0, len(files) - 1)\n",
    "        with open(output_dir + '/' + files[idx], 'rb') as infile:\n",
    "            dic = pickle.load(infile)\n",
    "        pad_width = max_len - dic['sample'].shape[1]\n",
    "        mfcc = np.pad(dic['sample'], pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        data_x.append(mfcc)\n",
    "        data_y.append(dic['embedding'])\n",
    "    data_x = np.asarray(data_x)\n",
    "    data_y = np.asarray(data_y)\n",
    "    data_x = np.expand_dims(data_x, axis=3)\n",
    "    return data_x, data_y\n",
    "\n",
    "print('training model')\n",
    "#encoder.fit(data_x, data_y, epochs=10, batch_size=32)\n",
    "model.fit_generator(data_generator(batch_size), steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "model.save('keras_asr.h5')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras_asr.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
